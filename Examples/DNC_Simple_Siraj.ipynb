{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Differentiable Neural Computer\n",
    "\n",
    "## The Problem - how do we create more general purpose learning machines?\n",
    "\n",
    "Neural networks excel at pattern recognition and quick, reactive decision-making, but we are only just \n",
    "beginning to build neural networks that can think slowly. that is, deliberate or reason using knowledge.\n",
    "For example, how could a neural network store memories for facts like the connections in a transport network \n",
    "and then logically reason about its pieces of knowledge to answer questions?\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/images/dnc_figure1.width-1500_Zfxk87k.png \"Logo Title Text 1\")\n",
    "\n",
    "this consists of a neural network that can read from and write to an external memory matrix,\n",
    "analogous to the random-access memory in a conventional computer.\n",
    "\n",
    "Like a conventional computer, it can use its memory to represent and manipulate complex data structures, \n",
    "but, like a neural network, it can learn to do so from data.\n",
    "\n",
    "DNCs have the capacity to solve complex, structured tasks that are \n",
    "inaccessible to neural networks without external read–write memory.\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/images/dnc_figure2.width-1500_be2TeKT.png \"Logo Title Text 1\")\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/B9U8sI7TcMY/0.jpg)](http://www.youtube.com/watch?v=B9U8sI7TcMYE)\n",
    "\n",
    "\n",
    "\n",
    "Modern computers separate computation and memory. Computation is performed by a processor, \n",
    "which can use an addressable memory to bring operands in and out of play. \n",
    "\n",
    "In contrast to computers, the computational and memory resources of artificial neural networks \n",
    "are mixed together in the network weights and neuron activity. This is a major liability: \n",
    "as the memory demands of a task increase, these networks cannot allocate new storage \n",
    "dynam-ically, nor easily learn algorithms that act independently of the values realized \n",
    "by the task variables.\n",
    "    \n",
    "The whole system is differentiable, and can therefore be trained \n",
    "end-to-end with gradient descent, allowing the network to learn \n",
    "how to operate and organize the memory in a goal-directed manner.\n",
    "\n",
    "If the memory can be thought of as the DNC’s RAM, then the network, referred to as the ‘controller’, \n",
    "is a differentiable CPU whose operations are learned with gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "How is it different from its predecessor, the Neural Turing Machine?\n",
    "\n",
    "basically, more memory access methods than NTM\n",
    "\n",
    " DNC extends the NTM addressing the following limitations:\n",
    "\n",
    "(1) Ensuring that blocks of allocated memory do not overlap and interfere.\n",
    "\n",
    "(2) Freeing memory that have already been written to.\n",
    "\n",
    "(3) Handling of non-contiguous memory through temporal links.\n",
    "\n",
    "\n",
    "the system required hand-crafted input to accomplish its learning and inference. This is not an NLP system where unstructured text is applied at input. \n",
    "\n",
    "3 forms of attention for heads\n",
    "- content lookup\n",
    "- records transitions between consecutively written locations in an N ×  N temporal link matrix L.\n",
    "This gives a DNC the native ability to recover sequences in the order in which it wrote them, even\n",
    "when consecutive writes did not occur in adjacent time-step\n",
    "- The third form of attention allocates memory for writing. \n",
    "\n",
    "Content lookup enables the formation of associative data structures;\n",
    "temporal links enable sequential retrieval of input sequences;\n",
    "and allocation provides the write head with unused locations. \n",
    "\n",
    "DNC memory modification is fast and can be one-shot, resembling the associative \n",
    "long-term potentiation of hippocampal CA3 and CA1 synapses\n",
    "\n",
    " Human ‘free recall’ experiments demonstrate the increased probability of \n",
    "   item recall in the same order as first pre-sented (temporal links)\n",
    "    \n",
    "DeepMind hopes that DNCs provide both a new tool for computer science and a new metaphor for cognitive science\n",
    "and neuroscience: here is a learning machine that, without prior programming, can organise information\n",
    "into connected facts and use those facts to solve problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC:\n",
    "    def __init__(self, input_size, output_size, seq_len, num_words = 256, word_size = 64, num_heads = 4):\n",
    "        '''\n",
    "        Initialize the DNC:\n",
    "        In this tutorial we are basically using the DNC to understand the mapping between the input\n",
    "        and output data.\n",
    "        \n",
    "        input data: [[0,0], [0,1], [1,0], [1,1]]\n",
    "        output data: [[1,0], [0,0], [0,0], [0,1]]\n",
    "        '''\n",
    "        # define input and output sizes\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # define read and write vectors\n",
    "        self.num_words = num_words # N\n",
    "        self.word_size = word_size # W\n",
    "        \n",
    "        # define number of read and write heads\n",
    "        self.num_heads = num_heads # R\n",
    "        \n",
    "        # size of output vector from controller\n",
    "        # the magic numbers are just a type of hyper-parameters\n",
    "        # set them according to your own use-case, they come from the way we divide our\n",
    "        # interface vector into read, write, gate and read mode variables\n",
    "        self.interface_size = num_heads*word_size + 3*word_size + 5*num_heads + 3\n",
    "        \n",
    "        # define input size\n",
    "        # this comes after the flatten the input and concatenate it with the\n",
    "        # previously read vectors from the memory\n",
    "        self.nn_input_size = num_heads*word_size + input_size\n",
    "        \n",
    "        # define output size\n",
    "        self.nn_output_size = output_size + self.interface_size\n",
    "        \n",
    "        # gaussian normal distribution for both outputs\n",
    "        # ???\n",
    "        self.nn_out = tf.truncated_normal([1, self.output_size], stddev = 0.1)\n",
    "        self.interface_vec = tf.truncated_normal([1, self.interface_size], stddev = 0.1)\n",
    "        \n",
    "        # define memory matrix\n",
    "        self.mem_mat = tf.zeros([num_words, word_size]) # N*W\n",
    "        \n",
    "        # define usage vector\n",
    "        # it tells which part of the memory have been used so far\n",
    "        self.usage_vec = tf.fill([num_words, 1], 1e-6) # W*1\n",
    "        \n",
    "        # define temporal link matrix\n",
    "        # it tells in which order the locations were written\n",
    "        self.link_mat = tf.zeros([num_words, num_words]) # N*N\n",
    "        \n",
    "        # define precedence weight\n",
    "        # it tell to the degree which the last weight was written to\n",
    "        self.precedence_weight = tf.zeros([num_words, 1]) # N*1\n",
    "        \n",
    "        # define read and write weight variables\n",
    "        self.read_weights = tf.fill([num_words, num_heads], 1e-6) # N*R\n",
    "        self.write_weights = tf.fill([num_words, 1], 1e-6) # N*1\n",
    "        self.read_vec = tf.fill([num_heads, word_size], 1e-6) # N*W\n",
    "        \n",
    "        #######################\n",
    "        ## Network Variables ##\n",
    "        #######################\n",
    "        \n",
    "        # parameters\n",
    "        hidden_layer_size = 32\n",
    "        \n",
    "        # define placeholders\n",
    "        self.i_data = tf.placeholder(tf.float32, [seq_len*2, self.input_size], name = 'input_placeholder')\n",
    "        self.o_data = tf.placeholder(tf.float32, [seq_len*2, self.output_size], name = 'output_placeholder')\n",
    "        \n",
    "        # define feedforward network weights\n",
    "        self.W1 = tf.Variable(tf.truncated_normal([self.nn_input_size, hidden_layer_size], stddev = 0.1),\n",
    "                              name = 'layer1_weights', dtype = tf.float32)\n",
    "        self.b1 = tf.Variable(tf.truncated_normal([hidden_layer_size], stddev = 0.1),\n",
    "                             name = 'layer1_bias', dtype = tf.float32)\n",
    "        self.W2 = tf.Variable(tf.truncated_normal([hidden_layer_size, self.nn_output_size], stddev = 0.1),\n",
    "                              name = 'layer2_weights', dtype = tf.float32) \n",
    "        self.b2 = tf.Variable(tf.truncated_normal([self.nn_output_size], stddev = 0.1),\n",
    "                             name = 'layer2_bias', dtype = tf.float32)\n",
    "        \n",
    "        # define DNC output weights\n",
    "        # self.nn_out_weights to convert the output of neural network into proper output\n",
    "        self.nn_out_weights = tf.Variable(\n",
    "            tf.truncated_normal([self.nn_output_size, self.output_size],\n",
    "                                stddev = 0.1),\n",
    "            name = 'nn_output_weights', dtype = tf.float32)\n",
    "        # self.interface_weights to convert the output of neural network to proper interface vector\n",
    "        self.interface_weights = tf.Variable(\n",
    "            tf.truncated_normal([self.nn_output_size, self.interface_size],\n",
    "                                stddev = 0.1),\n",
    "            name = 'interface_weights', dtype = tf.float32)\n",
    "        #\n",
    "        self.read_vec_out_weights = tf.Variable(\n",
    "            tf.truncated_normal([self.num_heads*self.word_size, self.output_size],\n",
    "                               stddev = 0.1),\n",
    "            name = 'read_vector_output_weights', dtype = tf.float32)\n",
    "        \n",
    "    ##########################\n",
    "    ## Attention Mechanisms ##\n",
    "    ##########################\n",
    "    '''\n",
    "    In DNC we have three different attention mechanisms:\n",
    "    1. Content Lookup (Content-Addressing in paper):\n",
    "        {From NTM paper} For content-addressing, each head (whether employed for reading or\n",
    "        writing) first produces a key-vector k, that is then compared to each vector in memory by\n",
    "        a similarity measure. The content-based system produces a normalized weighting based\n",
    "        on similarity [and a positive key-strength (beta), which can amplify or attenuate the\n",
    "        precision of the focus.]\n",
    "    2. Allocation weighting:\n",
    "        {From DNC paper} To allow controller to free and allocate memory as needed, we developed\n",
    "        a differentiable analogue to 'free-list' memory scheme, whereby a a list of available\n",
    "        memory location is maintained by adding to and removig from a linked list.\n",
    "        {From tutorial} The ‘usage’ of each location is represented as a number between 0 and 1,\n",
    "        and a weighting that picks out unused locations is delivered to the write head. This is\n",
    "        independent of the size and contents of the memory, meaning that  DNCs can be trained to\n",
    "        solve a task using one size of memory and later upgraded to a larger memory without\n",
    "        retraining\n",
    "    3. Temporal Linking:\n",
    "        {From DNC paper} The memory location defined [till now] stores no information about the\n",
    "        order in which memory locations are written to. However, there are many situation where\n",
    "        retaining this information is useful: for example, when a sequence inrtuctions must be\n",
    "        recorded and retrieved in order. We therefore use a temporal link matrix to keep track\n",
    "        of consecutively modified memory locations.\n",
    "    '''\n",
    "\n",
    "    # define content lookup\n",
    "    def content_lookup(self, key, str):\n",
    "        # str is 1*1 or 1*R\n",
    "        # l2 normalization of a vector is the square root of sum of absolute values squared\n",
    "        norm_mem = tf.nn.l2_normalize(self.mem_mat, 1) # N*W\n",
    "        norm_key = tf.nn.l2_normalize(key, 0) # 1*W for write, R*W for read\n",
    "        sim = tf.matmul(norm_mem, norm_key, transpose_b = True) # N*1 for write, N*R for read\n",
    "        return tf.nn.softmax(sim*str, 0) # N*1 or N*R\n",
    "\n",
    "    # define allocation weighting\n",
    "    def allocation_weighting(self):\n",
    "        # tf.nn.top_k() returns\n",
    "        # 1.The k largest elements along each last dimensional slice and\n",
    "        # 2.The indices of values within the last dimension of input\n",
    "        sorted_usage_vec, free_list = tf.nn.top_k(-1*self.usage_vec, k = self.num_words)\n",
    "        sorted_usage_vec *= -1\n",
    "        # tf.cumprod() calculates cumulative product\n",
    "        # tf.cumprod([a, b, c]) --> [a, a*b, a*b*c]\n",
    "        # tf.cumprod([a, b, c], exclusive=True) --> [1, a, a * b]\n",
    "        cumprod = tf.cumprod(sorted_usage_vec, axis = 0, exclusive = True)\n",
    "        unorder = (1-sorted_usage_vec)*cumprod\n",
    "\n",
    "        # allocation weight\n",
    "        alloc_weights = tf.zeros([self.num_words])\n",
    "        I = tf.constant(np.identity(self.num_words, dtype = np.float32))\n",
    "\n",
    "        # for each usage vector\n",
    "        for pos, idx in enumerate(tf.unstack(free_list[0])):\n",
    "            m = tf.squeeze(tf.slice(I, [idx, 0], [1, -1]))\n",
    "            alloc_weights += m*unorder[0, pos]\n",
    "\n",
    "        # allocation weighting for each row in memory\n",
    "        return tf.reshape(alloc_weights, [self.num_words, 1])\n",
    "\n",
    "    ###################\n",
    "    ## Step Function ##\n",
    "    ###################\n",
    "\n",
    "    # define the step function\n",
    "    '''\n",
    "    This is the function that we call while we are running our session at each iteration the\n",
    "    controller recieves two inputs that are concatenated, the input vector and the read vector\n",
    "    from previous time step it also gives two outputs, the output vector and the interface\n",
    "    vector that defines it's interaction with the memory at the current time step.\n",
    "    '''\n",
    "    def step_m(self, input_seq):\n",
    "        '''print('input_seq:',input_seq)\n",
    "        print('self.read_vec:', self.read_vec)\n",
    "        print('reshape',tf.reshape(self.read_vec, [1, self.num_words*self.word_size]))\n",
    "        print(tf.concat([input_seq, tf.reshape(self.read_vec, [1, self.num_heads*self.word_size])], 1))'''\n",
    "        # reshape the input\n",
    "        input_vec_nn = tf.concat([input_seq, tf.reshape(self.read_vec, [1, self.num_heads*self.word_size])], 1)\n",
    "\n",
    "        # forward propogation\n",
    "        l1_out = tf.matmul(input_vec_nn, self.W1) + self.b1\n",
    "        l1_act = tf.nn.tanh(l1_out)\n",
    "        l2_out = tf.matmul(l1_out, self.W2) + self.b2\n",
    "        l2_act = tf.nn.tanh(l2_out)\n",
    "\n",
    "        # output vector, the output of the DNC\n",
    "        self.nn_out = tf.matmul(l2_act, self.nn_out_weights)\n",
    "\n",
    "        # interface vector, how to interact with the memory\n",
    "        self.interface_vec = tf.matmul(l2_act, self.interface_weights)\n",
    "\n",
    "        # define partition vector\n",
    "        '''\n",
    "        We need to get lot of information from the interface vector, which will help us get various\n",
    "        vectors such as read vectors, write vectors, degree to which locations will be freed\n",
    "        '''\n",
    "        p_array = [0]*(self.num_heads * self.word_size) # read keys\n",
    "        p_array += [1]*(self.num_heads) # read string\n",
    "        p_array += [2]*(self.word_size) # write key\n",
    "        p_array += [3] # write string\n",
    "        p_array += [4]*(self.word_size) # erase vector\n",
    "        p_array += [5]*(self.word_size) # write vector\n",
    "        p_array += [6]*(self.num_heads) # free gates\n",
    "        p_array += [7] # allocation gates\n",
    "        p_array += [8] # write gates\n",
    "        p_array += [9]*(self.num_heads*3) # read mode\n",
    "        partition = tf.constant([p_array])\n",
    "\n",
    "        # convert interface vector to set of read write vectors\n",
    "        (read_keys, read_str, write_key, write_str, erase_vec,\n",
    "         write_vec, free_gates, alloc_gate, write_gate, read_modes) = \\\n",
    "        tf.dynamic_partition(self.interface_vec, partition, 10)\n",
    "\n",
    "        # read vectors\n",
    "        read_keys = tf.reshape(read_keys, [self.num_heads, self.word_size]) # R*W\n",
    "        read_str = 1 + tf.nn.softplus(tf.expand_dims(read_str, 0)) # 1*R\n",
    "\n",
    "        # write vectors\n",
    "        write_key = tf.expand_dims(write_key, 0) # 1*W\n",
    "        write_str = 1 + tf.nn.softplus(tf.expand_dims(write_str, 0)) # 1*1\n",
    "        erase_vec = tf.nn.sigmoid(tf.expand_dims(erase_vec, 0)) # 1*W\n",
    "        write_vec = tf.expand_dims(write_vec, 0) # 1*w\n",
    "\n",
    "        # gates\n",
    "        # free gates, the degree to which the locations at read head will be freed\n",
    "        free_gates = tf.nn.sigmoid(tf.expand_dims(free_gates, 0)) # 1*R\n",
    "        # the fraction of writing that is being allocated in a new location\n",
    "        alloc_gate = tf.nn.sigmoid(alloc_gate) # 1\n",
    "        # the amount of information to be written to memory\n",
    "        write_gate = tf.nn.sigmoid(write_gate) # 1\n",
    "\n",
    "        # read modes\n",
    "        # we do a softmax distribution between 3 read modes (backward, forward, lookup)\n",
    "        # The read heads can use gates called read modes to switch between content lookup \n",
    "        # using a read key and reading out locations either forwards or backwards \n",
    "        # in the order they were written.\n",
    "        read_modes = tf.nn.softmax(tf.reshape(read_modes, [3, self.num_heads])) # 3*R\n",
    "\n",
    "        ## WRITING\n",
    "\n",
    "        # the memory retention vector tells by how much each location will not be freed\n",
    "        # by the free gates, helps in determining usage vector\n",
    "        retention_vec = tf.reduce_prod(1 - free_gates*self.read_weights, reduction_indices = 1)\n",
    "        self.usage_vec = (self.usage_vec + self.write_weights - \\\n",
    "                          self.usage_vec*self.write_weights) * retention_vec\n",
    "\n",
    "        # allocation weighting is used to provide new locations for writing\n",
    "        alloc_weights = self.allocation_weighting()\n",
    "        write_lookup_weights = self.content_lookup(write_key, write_str)\n",
    "\n",
    "        # define write weights\n",
    "        self.write_weights = write_gate*(alloc_gate*alloc_weights + \\\n",
    "                            (1-alloc_gate)*write_lookup_weights)\n",
    "\n",
    "        # write -> erase -> write to memory\n",
    "        self.mem_mat = self.mem_mat*(1 - tf.matmul(self.write_weights, erase_vec)) + \\\n",
    "                            tf.matmul(self.write_weights, write_vec)\n",
    "            \n",
    "        # temporal link matrix\n",
    "        nnweight_vec = tf.matmul(self.write_weights, tf.ones([1, self.num_words])) # N*N\n",
    "        self.link_mat = self.link_mat*(1 - nnweight_vec - tf.transpose(nnweight_vec)) + \\\n",
    "                        tf.matmul(self.write_weights, self.precedence_weight, transpose_b = True)\n",
    "        self.link_mat *= tf.ones([self.num_words, self.num_words]) - \\\n",
    "                        tf.constant(np.identity(self.num_words, dtype = np.float32))\n",
    "            \n",
    "        # update precedence weight\n",
    "        self.precedence_weight = (1 - tf.reduce_sum(self.write_weights, reduction_indices = 0)) * \\\n",
    "                                self.precedence_weight + self.write_weights\n",
    "        \n",
    "        # 3 read modes\n",
    "        forw_w = read_modes[2] * tf.matmul(self.link_mat, self.read_weights)\n",
    "        look_w = read_modes[1] * self.content_lookup(read_keys, read_str)\n",
    "        back_w = read_modes[0] * tf.matmul(self.link_mat, self.read_weights, transpose_a = True)\n",
    "        \n",
    "        # initialize read weights\n",
    "        self.read_weights = forw_w + look_w + back_w\n",
    "        \n",
    "        # read vector\n",
    "        self.read_vec = tf.transpose(tf.matmul(self.mem_mat, self.read_weights, transpose_a = True))\n",
    "        \n",
    "        # get final read output\n",
    "        read_vec_mut = tf.matmul(tf.reshape(self.read_vec, [1, self.num_heads*self.word_size]),\n",
    "                                self.read_vec_out_weights)\n",
    "        \n",
    "        # return the final output\n",
    "        return self.nn_out + read_vec_mut\n",
    "    \n",
    "    # output the list of numbers (one hot encoded) by running step function\n",
    "    def run(self):\n",
    "        big_out = []\n",
    "        for t, seq in enumerate(tf.unstack(self.i_data, axis = 0)):\n",
    "            seq = tf.expand_dims(seq, 0)\n",
    "            y = self.step_m(seq)\n",
    "            big_out.append(y)\n",
    "        return tf.stack(big_out, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate randomly generated input, output sequences\n",
    "num_seq = 10\n",
    "seq_len = 6\n",
    "seq_width = 4\n",
    "num_epochs = 1000\n",
    "con = np.random.randint(0, seq_width,size=seq_len)\n",
    "seq = np.zeros((seq_len, seq_width))\n",
    "seq[np.arange(seq_len), con] = 1\n",
    "end = np.asarray([[-1]*seq_width])\n",
    "zer = np.zeros((seq_len, seq_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final i/o data\n",
    "final_i_data = np.concatenate((seq, zer), axis = 0)\n",
    "final_o_data = np.concatenate((zer, seq), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.716662\n",
      "100 0.291819\n",
      "200 0.139632\n",
      "300 0.102837\n",
      "400 0.0829762\n",
      "500 0.0597146\n",
      "600 0.034657\n",
      "700 0.0176897\n",
      "800 0.01247\n",
      "900 0.01024\n",
      "1000 0.00889811\n",
      "[3 1 1 3 3 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 3 1 1 3 3 0]\n",
      "[2 3 3 3 3 3 3 1 1 3 3 0]\n"
     ]
    }
   ],
   "source": [
    "# define compute graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "# running the graph\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # define the DNC\n",
    "        dnc = DNC(input_size = seq_width,\n",
    "                 output_size = seq_width,\n",
    "                 seq_len = seq_len,\n",
    "                 num_words = 10,\n",
    "                 word_size = 4,\n",
    "                 num_heads = 1)\n",
    "        \n",
    "        #calculate the predicted output\n",
    "        output = tf.squeeze(dnc.run())\n",
    "        #compare prediction to reality, get loss via sigmoid cross entropy\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=dnc.o_data))\n",
    "        #use regularizers for each layer of the controller\n",
    "        regularizers = (tf.nn.l2_loss(dnc.W1) + tf.nn.l2_loss(dnc.W2) +\n",
    "                        tf.nn.l2_loss(dnc.b1) + tf.nn.l2_loss(dnc.b2))\n",
    "        #to help the loss convergence faster\n",
    "        loss += 5e-4 * regularizers\n",
    "        #optimize the entire thing (memory + controller) using gradient descent. dope\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "        #initialize input output pairs\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #for each iteration\n",
    "        for i in range(0, num_epochs+1):\n",
    "            #feed in each input output pair\n",
    "            feed_dict = {dnc.i_data: final_i_data, dnc.o_data: final_o_data}\n",
    "            #make predictions\n",
    "            l, _, predictions = sess.run([loss, optimizer, output], feed_dict=feed_dict)\n",
    "            if i%100==0:\n",
    "                print(i,l)\n",
    "                \n",
    "        # print predictions\n",
    "        print(np.argmax(final_i_data, 1))\n",
    "        print(np.argmax(final_o_data, 1))\n",
    "        print(np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
